{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Processing output for 3D single person full-body tracking with FrankMocap </h1>\n",
    "\n",
    "<h3> \n",
    "    Wim Pouw ( wim.pouw@donders.ru.nl )<br>James Trujillo ( james.trujillo@donders.ru.nl )<br>\n",
    "    18-11-2021 </h3>\n",
    "    \n",
    "<img src=\"./images/envision_banner.png\"> </center>\n",
    "\n",
    "<h3> Info documents </h3>\n",
    "This module provides a demonstration of how to extract a time series from FrankMocap output. FrankMocap is a heavy-duty GPU-based single-person 3D motion tracking procedure that takes as input from 2D video and approximates 3D joint postions. So for it to run it (smoothly) you need a (strong) GPU (i.e., a good video card); see repository for specifications.\n",
    "\n",
    "FrankMocap is special in that it is dedicated for 3D motion tracking from 2D video. Where the lightweight Mediapipe body tracking also provides some depth information, FrankMocap is really designed for estimating as best as possible the 3D joint positions from 2D video. \n",
    "\n",
    "Note that FrankMocap only allows for tracking one person, and it will actually give an error if there are more than 1 person in the frame.\n",
    "\n",
    "The installation of frankmocap can be quite cumbersome, and the procedure depends on your operating system (for a description see the FrankMocap repository: ). We have installed frankmocap on a windows machine following the exact instructions of this helpful tutorial https://www.youtube.com/watch?v=MYLMM7jOMS4. If your institute has such resources you can ask your IT department to assist in the installation.\n",
    "\n",
    "But even when the installation is completed, the output you can then generate is not yet a time series and a tracking video that we can work with. FrankMocap generates only single video frames with the pose projected onto the frames, and per frame a .pkl file that contains tracking information. This python script therefore assists in this last step to transform the frankmocap output into workable time series data and create a video from the frames. We will extract the joint positions, but not that the script can be adapted if you want the 3D joint rotation angles instead. \n",
    "\n",
    "<h3> Executed code in conda command prompt </h3>\n",
    "We ran the full body tracking (hands and body) on the sample video to produce the motion tracking output by frankmocap.  \n",
    "\n",
    "> \"python -m demo.demo_frankmocap --input_path ./sampledata/TanDun_youtube_conductorexample.mp4 --out_dir ./mocap_output --save_pred_pkl\"\n",
    "\n",
    "<br><br>\n",
    "\n",
    "* location code: \n",
    "https://github.com/WimPouw/EnvisionBootcamp2021/tree/main/Python/FrankMocapBodyandHandTracking\n",
    "\n",
    "* citation: \n",
    "Pouw, W.  &  Trujillo, J.P.(2021-11-18). <i> Processing output for 3D single person full-body tracking with FrankMocap </i> \\[day you visited the site]. Retrieved from: https://github.com/WimPouw/EnvisionBootcamp2021/tree/main/Python/MediaBodyTracking \n",
    "\n",
    "<h4>resources</h4>\n",
    "* FrankMocap Github Repository: https://github.com/facebookresearch/frankmocap\n",
    "<br><br>\n",
    "* Rong, Y., Shiratori, T., & Joo, H. (2020). FrankMocap: Fast monocular 3D hand and body motion capture by regression and integration. arXiv preprint arXiv:2008.08324.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "#files that contains predicted points\n",
    "framefol = \"D:/frankmocap_win_install/mocap_output/mocap/\"\n",
    "framedata = os.listdir(framefol)\n",
    "\n",
    "#files that contain the images\n",
    "renderfol = 'D:/frankmocap_win_install/mocap_output/rendered/'\n",
    "renderdata = os.listdir(renderfol)\n",
    "\n",
    "#folder that contains the rendered video\n",
    "renderfold = 'RenderedVideos/'\n",
    "\n",
    "#output timeseries\n",
    "outtsfol = 'Output_timeseries/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = renderdata\n",
    "checkpixels = cv2.imread('D:/frankmocap_win_install/mocap_output/rendered/' + images[0])\n",
    "height, width = checkpixels.shape[:2]\n",
    "print(width, height)\n",
    "\n",
    "\n",
    "# choose codec according to format needed\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') \n",
    "video = cv2.VideoWriter(renderfold +'video.avi', fourcc, 29.97, (width, height))\n",
    "\n",
    "\n",
    "\n",
    "for i in images:\n",
    "    img = cv2.imread('D:/frankmocap_win_install/mocap_output/rendered/' + i)\n",
    "    video.write(img)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each left or right hand has\n",
    "#pred_joints_smpl\n",
    "#faces\n",
    "#bbox_scale_ratio\n",
    "#bboc_top_left\n",
    "#pred_camera\n",
    "#img_cropped\n",
    "#pred_hand_pose -> this gives the joint angles\n",
    "#pred_hand_betas \n",
    "#pred_joints_img -> this gives the joint positions (rescaled for camera)\n",
    "\n",
    "#https://github.com/facebookresearch/frankmocap/issues/31\n",
    "#in body+hand mode the body points are the same as openpose\n",
    "markerspositionhand = ['Wrist', 'Thumb_00', 'Thumb_01', 'Thumb_02', 'Thumb_03', 'Index_00', 'Index_01',\n",
    "                'Index_02', 'Index_03', 'Middle_00', 'Middle_01', 'Middle_02', 'Middle_03', 'Ring_00',\n",
    "                'Ring_01', 'Ring_02', 'Ring_03', 'Little_00', 'Little_01', 'Little_02', 'Little_03']\n",
    "\n",
    "markerhands = []\n",
    "for i in ['L_', 'R_']:\n",
    "    for j in markerspositionhand:\n",
    "        for k in ['x_', 'y_', 'z_']:\n",
    "            markerhands.append(str.lower(k + i + j))\n",
    "\n",
    "#for upper body we only want these points and they for position they follow openpose       \n",
    "markerspositionbody = ['Nose', 'Neck', 'Rshoulder', 'Relbow', 'RWrist', 'LShoulder',\n",
    "               'LElbow', 'LWrist', 'Midhip', 'RHip', 'RKnee', 'RAnkle', 'LHip']\n",
    "              #'LKnee', 'LAnkle', 'REye', 'LEye', 'REar', 'LEar', 'LBigToe', 'LSmallToe', \n",
    "              # 'LHeel', 'RBigToe', 'RSmallToe', 'RHeel', 'Background']\n",
    "\n",
    "markerbody = []\n",
    "for j in markerspositionbody:\n",
    "    for k in ['x_', 'y_', 'z_']:\n",
    "        markerbody.append(str.lower(k + j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = markerhands+markerbody\n",
    "\n",
    "timeseries = []\n",
    "timeseries.append(columns) #the first is a list with columns\n",
    "for frame in framedata:\n",
    "    with open(framefol+frame, 'rb') as f:\n",
    "        data = pickle.load(f)    \n",
    "    timsdat = data['pred_output_list']\n",
    "    timsdat = pd.DataFrame(timsdat)\n",
    "    #extract body joint positions\n",
    "    leftjoints = np.ndarray.tolist(timsdat['pred_lhand_joints_img'][0])\n",
    "    rightjoints = np.ndarray.tolist(timsdat['pred_rhand_joints_img'][0])\n",
    "    bodyjoints = np.ndarray.tolist(timsdat['pred_body_joints_img'][0])\n",
    "    #get everything in a flat list\n",
    "    lj = []\n",
    "    rj = []\n",
    "    bj = []\n",
    "    for i in leftjoints:\n",
    "        lj.extend(i)\n",
    "    for i in leftjoints:\n",
    "        rj.extend(i)\n",
    "    it = 0\n",
    "    for i in markerspositionbody:\n",
    "        bj.extend(bodyjoints[it])\n",
    "        it+=1\n",
    "    #append to the the list\n",
    "    timeseries.append(lj+rj+bj)\n",
    "#bodyangles = timsdat['pred_body_pose'][0][0]\n",
    "#leftjoints = timsdat['pred_rhand_joints_img'][0]\n",
    "#leftangles = timsdat['pred_left_hand_pose'][0]\n",
    "#rightjoints = timsdat['pred_right_joints_img'][0]\n",
    "#rightangles = timsdat['pred_right_hand_pose'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### data to be written row-wise in csv fil\n",
    "data = timeseries\n",
    "  \n",
    "# opening the csv file in 'w+' mode\n",
    "file = open(outtsfol + 'timeseries.csv', 'w+', newline ='')\n",
    "#write it\n",
    "with file:    \n",
    "    write = csv.writer(file)\n",
    "    write.writerows(data)\n",
    "print('done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
